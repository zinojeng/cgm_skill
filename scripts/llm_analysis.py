#!/usr/bin/env python3
"""
CGM LLM æ™ºèƒ½åˆ†æå ±å‘Šç”Ÿæˆå™¨
ä½¿ç”¨ OpenAI API ç”Ÿæˆå€‹äººåŒ–çš„è¡€ç³–ç®¡ç†å»ºè­°
"""

import json
import sys
import os
from typing import Dict, List, Optional
from datetime import datetime
import pandas as pd
from openai import OpenAI

class CGMLLMAnalyzer:
    """CGM LLM åˆ†æå™¨"""

    def __init__(self, api_key: str, model: str = "gpt-4o"):
        """
        åˆå§‹åŒ– LLM åˆ†æå™¨

        Args:
            api_key: OpenAI API é‡‘é‘°
            model: ä½¿ç”¨çš„æ¨¡å‹åç¨±
        """
        self.client = OpenAI(api_key=api_key)
        self.model = model
        self.analysis_results = {}

    def load_metrics(self, metrics_file: str) -> Dict:
        """è¼‰å…¥åˆ†ææŒ‡æ¨™"""
        with open(metrics_file, 'r', encoding='utf-8') as f:
            return json.load(f)

    def load_knowledge(self, knowledge_dir: str = "../knowledge") -> str:
        """è¼‰å…¥å°ˆæ¥­çŸ¥è­˜åº«"""
        knowledge_content = ""

        # è¼‰å…¥ CGM æŒ‡æ¨™åƒè€ƒ
        metrics_file = os.path.join(knowledge_dir, "cgm_metrics.md")
        if os.path.exists(metrics_file):
            with open(metrics_file, 'r', encoding='utf-8') as f:
                knowledge_content += f"ã€CGM æŒ‡æ¨™åƒè€ƒã€‘\n{f.read()}\n\n"

        # è¼‰å…¥ç³–å°¿ç—…ç®¡ç†æŒ‡å—
        guidelines_file = os.path.join(knowledge_dir, "diabetes_guidelines.md")
        if os.path.exists(guidelines_file):
            with open(guidelines_file, 'r', encoding='utf-8') as f:
                knowledge_content += f"ã€ç³–å°¿ç—…ç®¡ç†æŒ‡å—ã€‘\n{f.read()}\n\n"

        return knowledge_content

    def analyze_agp_pattern(self, metrics: Dict) -> str:
        """åˆ†æ AGP æ¨¡å¼"""
        prompt = f"""
        æ ¹æ“šä»¥ä¸‹ CGM æ•¸æ“šçš„æ¯å°æ™‚è¡€ç³–æ¨¡å¼ï¼Œåˆ†æè¡€ç³–æ§åˆ¶çš„ç‰¹å¾µå’Œå•é¡Œï¼š

        æ¯å°æ™‚å¹³å‡è¡€ç³–ï¼ˆmg/dLï¼‰ï¼š
        {json.dumps(metrics.get('Hourly Pattern', {}), indent=2)}

        æ•´é«”æŒ‡æ¨™ï¼š
        - å¹³å‡è¡€ç³–ï¼š{metrics.get('Mean Glucose', 0):.1f} mg/dL
        - CVï¼š{metrics.get('CV', 0):.1f}%
        - SDï¼š{metrics.get('SD', 0):.1f} mg/dL

        è«‹åˆ†æï¼š
        1. è­˜åˆ¥è¡€ç³–æ¨¡å¼ï¼ˆæ™¨æ›¦ç¾è±¡ã€å¤œé–“ä½è¡€ç³–é¢¨éšªç­‰ï¼‰
        2. æ‰¾å‡ºè¡€ç³–æ§åˆ¶çš„å•é¡Œæ™‚æ®µ
        3. è©•ä¼°è¡€ç³–è®Šç•°æ€§
        4. æä¾›å…·é«”çš„æ”¹å–„å»ºè­°

        è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œä¸¦ä½¿ç”¨å°ˆæ¥­ä½†æ˜“æ‡‚çš„èªè¨€ã€‚
        """

        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ç³–å°¿ç—…ç®¡ç†å°ˆå®¶ï¼Œæ“…é•·åˆ†æ CGM æ•¸æ“šä¸¦æä¾›å€‹äººåŒ–å»ºè­°ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=1500
        )

        return response.choices[0].message.content

    def analyze_time_in_range(self, metrics: Dict) -> str:
        """åˆ†æ Time in Range"""
        prompt = f"""
        è«‹åˆ†æä»¥ä¸‹ CGM Time in Range æ•¸æ“šï¼š

        - TIR (70-180 mg/dL): {metrics.get('TIR', 0):.1f}%
        - TAR (>180 mg/dL): {metrics.get('TAR', 0):.1f}%
        - TBR (<70 mg/dL): {metrics.get('TBR', 0):.1f}%
        - Very Low (<54 mg/dL): {metrics.get('Very Low (<54)', 0):.1f}%
        - Very High (>250 mg/dL): {metrics.get('Very High (>250)', 0):.1f}%

        GMI: {metrics.get('GMI', 0):.1f}%
        GRI: {metrics.get('GRI', 0):.1f}

        åœ‹éš›ç›®æ¨™ï¼š
        - TIR >70%
        - TBR <4%
        - TAR <25%

        è«‹æä¾›ï¼š
        1. èˆ‡åœ‹éš›æ¨™æº–çš„æ¯”è¼ƒ
        2. å„ªå…ˆæ”¹å–„çš„é …ç›®
        3. å…·é«”çš„èª¿æ•´å»ºè­°
        4. é¢¨éšªè©•ä¼°

        è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚
        """

        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ç¶“é©—è±å¯Œçš„å…§åˆ†æ³Œç§‘é†«å¸«ï¼Œå°ˆç²¾æ–¼ç³–å°¿ç—…ç®¡ç†ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=1500
        )

        return response.choices[0].message.content

    def generate_personalized_recommendations(self, metrics: Dict, patient_profile: Optional[Dict] = None) -> str:
        """ç”Ÿæˆå€‹äººåŒ–å»ºè­°"""
        profile_info = ""
        if patient_profile:
            profile_info = f"""
            æ‚£è€…è³‡æ–™ï¼š
            - ç³–å°¿ç—…é¡å‹ï¼š{patient_profile.get('diabetes_type', 'æœªæŒ‡å®š')}
            - å¹´é½¡çµ„ï¼š{patient_profile.get('age_group', 'æˆäºº')}
            - æ²»ç™‚æ–¹æ¡ˆï¼š{patient_profile.get('treatment', 'æœªæŒ‡å®š')}
            """

        prompt = f"""
        åŸºæ–¼ä»¥ä¸‹ CGM åˆ†æçµæœï¼Œæä¾›å€‹äººåŒ–çš„è¡€ç³–ç®¡ç†å»ºè­°ï¼š

        {profile_info}

        é—œéµæŒ‡æ¨™ï¼š
        - å¹³å‡è¡€ç³–ï¼š{metrics.get('Mean Glucose', 0):.1f} mg/dL
        - CVï¼š{metrics.get('CV', 0):.1f}%
        - TIRï¼š{metrics.get('TIR', 0):.1f}%
        - GMIï¼š{metrics.get('GMI', 0):.1f}%
        - GRIï¼š{metrics.get('GRI', 0):.1f}

        è«‹æä¾›ä»¥ä¸‹æ–¹é¢çš„å…·é«”å»ºè­°ï¼š

        1. ã€èƒ°å³¶ç´ èª¿æ•´ã€‘
           - åŸºç¤èƒ°å³¶ç´ èª¿æ•´å»ºè­°
           - é¤æ™‚èƒ°å³¶ç´ å„ªåŒ–
           - æ ¡æ­£åŠ‘é‡ç­–ç•¥

        2. ã€é£²é£Ÿç®¡ç†ã€‘
           - ç¢³æ°´åŒ–åˆç‰©æ”å–å»ºè­°
           - ç”¨é¤æ™‚é–“å®‰æ’
           - ç‰¹å®šé£Ÿç‰©é¸æ“‡

        3. ã€ç”Ÿæ´»æ–¹å¼ã€‘
           - é‹å‹•æ™‚æ©Ÿå’Œé¡å‹
           - ç¡çœ å°è¡€ç³–çš„å½±éŸ¿
           - å£“åŠ›ç®¡ç†

        4. ã€ç›£æ¸¬é‡é»ã€‘
           - éœ€è¦åŠ å¼·ç›£æ¸¬çš„æ™‚æ®µ
           - ç‰¹æ®Šæƒ…æ³çš„è™•ç†
           - CGM ä½¿ç”¨å„ªåŒ–

        5. ã€é¢¨éšªé é˜²ã€‘
           - ä½è¡€ç³–é é˜²æªæ–½
           - é«˜è¡€ç³–è™•ç†æ–¹æ¡ˆ
           - ç·Šæ€¥æƒ…æ³æº–å‚™

        è«‹æä¾›å¯¦ç”¨ã€å…·é«”ã€å¯åŸ·è¡Œçš„å»ºè­°ï¼Œä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚
        """

        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½è³‡æ·±çš„ç³–å°¿ç—…ç…§è­·åœ˜éšŠæˆå“¡ï¼ŒåŒ…å«é†«å¸«ã€ç‡Ÿé¤Šå¸«å’Œè¡›æ•™å¸«çš„å°ˆæ¥­çŸ¥è­˜ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=2000
        )

        return response.choices[0].message.content

    def generate_comprehensive_report(self, metrics: Dict, output_file: str = "llm_report.md"):
        """ç”Ÿæˆå®Œæ•´çš„ LLM åˆ†æå ±å‘Š"""
        print("æ­£åœ¨ç”Ÿæˆ AI åˆ†æå ±å‘Š...")

        # è¼‰å…¥å°ˆæ¥­çŸ¥è­˜ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        knowledge = self.load_knowledge()

        # åŸ·è¡Œå„é …åˆ†æ
        print("  åˆ†æ AGP æ¨¡å¼...")
        agp_analysis = self.analyze_agp_pattern(metrics)

        print("  åˆ†æ Time in Range...")
        tir_analysis = self.analyze_time_in_range(metrics)

        print("  ç”Ÿæˆå€‹äººåŒ–å»ºè­°...")
        recommendations = self.generate_personalized_recommendations(metrics)

        # çµ„åˆå ±å‘Š
        report = f"""# CGM æ™ºèƒ½åˆ†æå ±å‘Š

ç”Ÿæˆæ™‚é–“ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
åˆ†ææ¨¡å‹ï¼š{self.model}

---

## ğŸ“Š æ•¸æ“šæ‘˜è¦

- **å¹³å‡è¡€ç³–**ï¼š{metrics.get('Mean Glucose', 0):.1f} mg/dL
- **è®Šç•°ä¿‚æ•¸ (CV)**ï¼š{metrics.get('CV', 0):.1f}%
- **Time in Range**ï¼š{metrics.get('TIR', 0):.1f}%
- **GMI**ï¼š{metrics.get('GMI', 0):.1f}%
- **GRI**ï¼š{metrics.get('GRI', 0):.1f}

---

## ğŸ” AGP æ¨¡å¼åˆ†æ

{agp_analysis}

---

## ğŸ“ˆ Time in Range è©•ä¼°

{tir_analysis}

---

## ğŸ’¡ å€‹äººåŒ–ç®¡ç†å»ºè­°

{recommendations}

---

## âš ï¸ é‡è¦æé†’

1. **é†«ç™‚è«®è©¢**ï¼šæœ¬å ±å‘Šåƒ…ä¾›åƒè€ƒï¼Œæ‰€æœ‰æ²»ç™‚èª¿æ•´æ‡‰åœ¨é†«ç™‚å°ˆæ¥­äººå“¡æŒ‡å°ä¸‹é€²è¡Œ
2. **æŒçºŒç›£æ¸¬**ï¼šå»ºè­°å®šæœŸï¼ˆæ¯2-4é€±ï¼‰é‡æ–°è©•ä¼°è¡€ç³–æ§åˆ¶æƒ…æ³
3. **å€‹é«”å·®ç•°**ï¼šæ¯å€‹äººå°æ²»ç™‚çš„åæ‡‰ä¸åŒï¼Œéœ€è¦å€‹é«”åŒ–èª¿æ•´
4. **å®‰å…¨ç¬¬ä¸€**ï¼šå„ªå…ˆè™•ç†ä½è¡€ç³–é¢¨éšªï¼Œå†å„ªåŒ–é«˜è¡€ç³–æ§åˆ¶

---

## ğŸ“š åƒè€ƒè³‡æº

- [ç¾åœ‹ç³–å°¿ç—…å”æœƒ (ADA) æ¨™æº–](https://www.diabetes.org)
- [åœ‹éš›ç³–å°¿ç—…è¯ç›Ÿ (IDF)](https://www.idf.org)
- [CGM ä½¿ç”¨å…±è­˜è²æ˜](https://doi.org/10.2337/dci19-0062)

---

*æœ¬å ±å‘Šç”± AI è¼”åŠ©ç”Ÿæˆï¼Œçµåˆäº†æœ€æ–°çš„ç³–å°¿ç—…ç®¡ç†æŒ‡å—å’Œ CGM æ•¸æ“šåˆ†ææœ€ä½³å¯¦è¸ã€‚*
"""

        # å„²å­˜å ±å‘Š
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report)

        print(f"âœ“ AI åˆ†æå ±å‘Šå·²å„²å­˜è‡³ï¼š{output_file}")

        # åŒæ™‚å„²å­˜ç‚º JSON æ ¼å¼
        json_file = output_file.replace('.md', '.json')
        analysis_json = {
            "timestamp": datetime.now().isoformat(),
            "model": self.model,
            "metrics": metrics,
            "analysis": {
                "agp_pattern": agp_analysis,
                "time_in_range": tir_analysis,
                "recommendations": recommendations
            }
        }

        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_json, f, indent=2, ensure_ascii=False)

        print(f"âœ“ JSON æ ¼å¼å ±å‘Šå·²å„²å­˜è‡³ï¼š{json_file}")

        return report

    def analyze_insulin_patterns(self, insulin_data: pd.DataFrame, metrics: Dict) -> str:
        """åˆ†æèƒ°å³¶ç´ ä½¿ç”¨æ¨¡å¼ï¼ˆå¦‚æœæœ‰æ•¸æ“šï¼‰"""
        if insulin_data is None or insulin_data.empty:
            return "ç„¡èƒ°å³¶ç´ æ•¸æ“šå¯ä¾›åˆ†æ"

        # çµ±è¨ˆèƒ°å³¶ç´ ä½¿ç”¨
        insulin_summary = {
            "total_injections": len(insulin_data),
            "daily_average": len(insulin_data) / metrics.get('Daily Stats', {}).get('Days Analyzed', 1),
            "types": insulin_data['Event Subtype'].value_counts().to_dict() if 'Event Subtype' in insulin_data.columns else {}
        }

        prompt = f"""
        åˆ†æä»¥ä¸‹èƒ°å³¶ç´ ä½¿ç”¨æ•¸æ“šä¸¦æä¾›å„ªåŒ–å»ºè­°ï¼š

        èƒ°å³¶ç´ ä½¿ç”¨çµ±è¨ˆï¼š
        {json.dumps(insulin_summary, indent=2, ensure_ascii=False)}

        è¡€ç³–æ§åˆ¶çµæœï¼š
        - TIR: {metrics.get('TIR', 0):.1f}%
        - å¹³å‡è¡€ç³–: {metrics.get('Mean Glucose', 0):.1f} mg/dL
        - CV: {metrics.get('CV', 0):.1f}%

        è«‹åˆ†æï¼š
        1. èƒ°å³¶ç´ ä½¿ç”¨æ¨¡å¼æ˜¯å¦åˆç†
        2. èˆ‡è¡€ç³–æ§åˆ¶çš„ç›¸é—œæ€§
        3. å¯èƒ½çš„å„ªåŒ–æ–¹å‘
        4. å…·é«”èª¿æ•´å»ºè­°

        ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚
        """

        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½èƒ°å³¶ç´ æ²»ç™‚å°ˆå®¶ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=1000
        )

        return response.choices[0].message.content

def main():
    """ä¸»å‡½æ•¸"""
    if len(sys.argv) < 3:
        print("ä½¿ç”¨æ–¹æ³•ï¼špython llm_analysis.py <metrics.json> <api_key> [model]")
        print("ç¯„ä¾‹ï¼špython llm_analysis.py metrics.json sk-xxx gpt-4o")
        sys.exit(1)

    metrics_file = sys.argv[1]
    api_key = sys.argv[2]
    model = sys.argv[3] if len(sys.argv) > 3 else "gpt-4o"

    if not os.path.exists(metrics_file):
        print(f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æª”æ¡ˆ {metrics_file}")
        sys.exit(1)

    print("=" * 60)
    print("CGM LLM æ™ºèƒ½åˆ†æ")
    print("=" * 60)

    try:
        # åˆå§‹åŒ–åˆ†æå™¨
        analyzer = CGMLLMAnalyzer(api_key, model)

        # è¼‰å…¥æŒ‡æ¨™
        print("è¼‰å…¥åˆ†ææŒ‡æ¨™...")
        metrics = analyzer.load_metrics(metrics_file)

        # ç”Ÿæˆå ±å‘Š
        print("é–‹å§‹ AI åˆ†æ...")
        report = analyzer.generate_comprehensive_report(metrics)

        print("\nâœ… AI åˆ†æå®Œæˆï¼")

    except Exception as e:
        print(f"éŒ¯èª¤ï¼š{str(e)}")
        sys.exit(1)

    return 0

if __name__ == "__main__":
    main()