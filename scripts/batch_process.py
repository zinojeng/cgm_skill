#!/usr/bin/env python3
"""
CGM æ‰¹æ¬¡è™•ç†å·¥å…·
è™•ç†å¤šå€‹ CGM æª”æ¡ˆä¸¦ç”Ÿæˆæ¯”è¼ƒå ±å‘Š
"""

import os
import sys
import glob
import json
import yaml
from pathlib import Path
from datetime import datetime
from concurrent.futures import ProcessPoolExecutor, as_completed
from typing import Dict, List, Optional, Tuple
import pandas as pd
import numpy as np
from tqdm import tqdm

# åŒ¯å…¥å…¶ä»–æ¨¡çµ„
sys.path.append(os.path.dirname(__file__))
from analyze_cgm import CGMAnalyzer
from validate import CGMValidator


class BatchProcessor:
    """æ‰¹æ¬¡è™•ç†å™¨"""

    def __init__(self, config_file: Optional[str] = None):
        """åˆå§‹åŒ–æ‰¹æ¬¡è™•ç†å™¨"""
        self.config = self._load_config(config_file)
        self.results = []
        self.errors = []

    def _load_config(self, config_file: Optional[str]) -> Dict:
        """è¼‰å…¥é…ç½®"""
        default_config = {
            "max_parallel": 4,
            "batch_size": 10,
            "output_dir": "./batch_output",
            "validate_first": True,
            "generate_comparison": True
        }

        if config_file and os.path.exists(config_file):
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    if config_file.endswith('.yaml'):
                        user_config = yaml.safe_load(f)
                    else:
                        user_config = json.load(f)

                # åˆä½µé…ç½®
                if "batch_processing" in user_config:
                    default_config.update(user_config["batch_processing"])

            except Exception as e:
                print(f"âš ï¸ ç„¡æ³•è¼‰å…¥é…ç½®æª”æ¡ˆï¼š{e}")

        return default_config

    def process_files(self, file_pattern: str) -> Dict:
        """è™•ç†å¤šå€‹æª”æ¡ˆ"""
        # å°‹æ‰¾åŒ¹é…çš„æª”æ¡ˆ
        files = glob.glob(file_pattern)

        if not files:
            print(f"âŒ æ‰¾ä¸åˆ°åŒ¹é…çš„æª”æ¡ˆï¼š{file_pattern}")
            return {"status": "error", "message": "No files found"}

        print(f"æ‰¾åˆ° {len(files)} å€‹æª”æ¡ˆå¾…è™•ç†")

        # å‰µå»ºè¼¸å‡ºç›®éŒ„
        os.makedirs(self.config["output_dir"], exist_ok=True)

        # é©—è­‰æª”æ¡ˆï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        if self.config["validate_first"]:
            print("\nğŸ“‹ é©—è­‰æª”æ¡ˆ...")
            valid_files = self._validate_files(files)

            if not valid_files:
                print("âŒ æ²’æœ‰æœ‰æ•ˆçš„æª”æ¡ˆå¯è™•ç†")
                return {"status": "error", "message": "No valid files"}

            files = valid_files

        # æ‰¹æ¬¡è™•ç†
        print(f"\nğŸ”„ é–‹å§‹æ‰¹æ¬¡è™•ç†ï¼ˆæœ€å¤§ä¸¦è¡Œï¼š{self.config['max_parallel']}ï¼‰...")

        with ProcessPoolExecutor(max_workers=self.config["max_parallel"]) as executor:
            # æäº¤ä»»å‹™
            futures = {}
            for file_path in files:
                future = executor.submit(self._process_single_file, file_path)
                futures[future] = file_path

            # è™•ç†çµæœ
            with tqdm(total=len(files), desc="è™•ç†é€²åº¦") as pbar:
                for future in as_completed(futures):
                    file_path = futures[future]

                    try:
                        result = future.result(timeout=300)  # 5 åˆ†é˜è¶…æ™‚
                        self.results.append(result)
                    except Exception as e:
                        error = {
                            "file": file_path,
                            "error": str(e)
                        }
                        self.errors.append(error)
                        print(f"\nâŒ è™•ç†å¤±æ•—ï¼š{file_path} - {e}")

                    pbar.update(1)

        # ç”Ÿæˆæ¯”è¼ƒå ±å‘Š
        if self.config["generate_comparison"] and len(self.results) > 1:
            print("\nğŸ“Š ç”Ÿæˆæ¯”è¼ƒå ±å‘Š...")
            self._generate_comparison_report()

        # ç”Ÿæˆç¸½çµå ±å‘Š
        summary = self._generate_summary()

        print("\nâœ… æ‰¹æ¬¡è™•ç†å®Œæˆï¼")
        print(f"æˆåŠŸï¼š{len(self.results)} å€‹æª”æ¡ˆ")
        print(f"å¤±æ•—ï¼š{len(self.errors)} å€‹æª”æ¡ˆ")

        return summary

    def _validate_files(self, files: List[str]) -> List[str]:
        """é©—è­‰æª”æ¡ˆ"""
        valid_files = []

        for file_path in files:
            validator = CGMValidator(file_path)
            result = validator.validate()

            if result["is_valid"]:
                valid_files.append(file_path)
                print(f"  âœ“ {os.path.basename(file_path)} - é©—è­‰é€šé")
            else:
                print(f"  âœ— {os.path.basename(file_path)} - é©—è­‰å¤±æ•—")
                for error in result["errors"]:
                    print(f"    - {error}")

        return valid_files

    def _process_single_file(self, file_path: str) -> Dict:
        """è™•ç†å–®å€‹æª”æ¡ˆ"""
        try:
            # å‰µå»ºåˆ†æå™¨
            analyzer = CGMAnalyzer(file_path)

            # è¨ˆç®—æŒ‡æ¨™
            metrics = analyzer.calculate_metrics()

            # ç”Ÿæˆå€‹äººå ±å‘Šç›®éŒ„
            file_name = Path(file_path).stem
            output_dir = os.path.join(self.config["output_dir"], file_name)
            os.makedirs(output_dir, exist_ok=True)

            # ç”Ÿæˆå ±å‘Š
            analyzer.generate_report(output_dir)

            # è¿”å›çµæœ
            return {
                "file": file_path,
                "name": file_name,
                "status": "success",
                "metrics": metrics,
                "output_dir": output_dir
            }

        except Exception as e:
            return {
                "file": file_path,
                "status": "error",
                "error": str(e)
            }

    def _generate_comparison_report(self):
        """ç”Ÿæˆæ¯”è¼ƒå ±å‘Š"""
        comparison_data = []

        # æ”¶é›†æ‰€æœ‰æˆåŠŸè™•ç†çš„çµæœ
        for result in self.results:
            if result["status"] == "success":
                row = {
                    "File": result["name"],
                    "Mean Glucose": result["metrics"].get("Mean Glucose", np.nan),
                    "TIR (%)": result["metrics"].get("TIR", np.nan),
                    "TAR (%)": result["metrics"].get("TAR", np.nan),
                    "TBR (%)": result["metrics"].get("TBR", np.nan),
                    "CV (%)": result["metrics"].get("CV", np.nan),
                    "GMI": result["metrics"].get("GMI", np.nan),
                    "GRI": result["metrics"].get("GRI", np.nan)
                }
                comparison_data.append(row)

        if not comparison_data:
            return

        # å‰µå»º DataFrame
        df = pd.DataFrame(comparison_data)

        # è¨ˆç®—çµ±è¨ˆ
        summary_stats = df.describe()

        # ç”Ÿæˆ Markdown å ±å‘Š
        report_file = os.path.join(self.config["output_dir"], "comparison_report.md")

        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# CGM æ‰¹æ¬¡åˆ†ææ¯”è¼ƒå ±å‘Š\n\n")
            f.write(f"ç”Ÿæˆæ™‚é–“ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write(f"åˆ†ææª”æ¡ˆæ•¸ï¼š{len(comparison_data)}\n\n")

            # è©³ç´°æ•¸æ“šè¡¨
            f.write("## è©³ç´°æ•¸æ“š\n\n")
            f.write(df.to_markdown(index=False, floatfmt=".1f"))
            f.write("\n\n")

            # çµ±è¨ˆæ‘˜è¦
            f.write("## çµ±è¨ˆæ‘˜è¦\n\n")
            f.write(summary_stats.to_markdown(floatfmt=".1f"))
            f.write("\n\n")

            # æœ€ä½³å’Œæœ€å·®è¡¨ç¾
            f.write("## è¡¨ç¾åˆ†æ\n\n")

            if "TIR (%)" in df.columns:
                best_tir = df.nlargest(1, "TIR (%)")
                worst_tir = df.nsmallest(1, "TIR (%)")

                f.write(f"**æœ€ä½³ TIRï¼š** {best_tir['File'].values[0]} ")
                f.write(f"({best_tir['TIR (%)'].values[0]:.1f}%)\n\n")

                f.write(f"**æœ€å·® TIRï¼š** {worst_tir['File'].values[0]} ")
                f.write(f"({worst_tir['TIR (%)'].values[0]:.1f}%)\n\n")

            if "CV (%)" in df.columns:
                best_cv = df.nsmallest(1, "CV (%)")
                worst_cv = df.nlargest(1, "CV (%)")

                f.write(f"**æœ€ç©©å®šï¼ˆæœ€ä½ CVï¼‰ï¼š** {best_cv['File'].values[0]} ")
                f.write(f"({best_cv['CV (%)'].values[0]:.1f}%)\n\n")

                f.write(f"**æœ€ä¸ç©©å®šï¼ˆæœ€é«˜ CVï¼‰ï¼š** {worst_cv['File'].values[0]} ")
                f.write(f"({worst_cv['CV (%)'].values[0]:.1f}%)\n\n")

            # å»ºè­°
            f.write("## å»ºè­°\n\n")

            # æ‰¾å‡ºéœ€è¦æ”¹å–„çš„æª”æ¡ˆ
            need_improvement = []

            for _, row in df.iterrows():
                issues = []

                if row.get("TIR (%)", 100) < 70:
                    issues.append("TIR ä½æ–¼ç›®æ¨™")
                if row.get("TBR (%)", 0) > 4:
                    issues.append("ä½è¡€ç³–æ™‚é–“éå¤š")
                if row.get("CV (%)", 0) > 36:
                    issues.append("è¡€ç³–è®Šç•°æ€§éé«˜")

                if issues:
                    need_improvement.append({
                        "file": row["File"],
                        "issues": issues
                    })

            if need_improvement:
                f.write("### éœ€è¦é—œæ³¨çš„æª”æ¡ˆï¼š\n\n")
                for item in need_improvement:
                    f.write(f"- **{item['file']}**\n")
                    for issue in item["issues"]:
                        f.write(f"  - {issue}\n")
                    f.write("\n")

        # å„²å­˜ CSV æ ¼å¼
        csv_file = os.path.join(self.config["output_dir"], "comparison_data.csv")
        df.to_csv(csv_file, index=False, encoding='utf-8')

        print(f"  âœ“ æ¯”è¼ƒå ±å‘Šå·²ç”Ÿæˆï¼š{report_file}")

    def _generate_summary(self) -> Dict:
        """ç”Ÿæˆç¸½çµ"""
        summary = {
            "total_files": len(self.results) + len(self.errors),
            "successful": len(self.results),
            "failed": len(self.errors),
            "output_directory": self.config["output_dir"],
            "timestamp": datetime.now().isoformat()
        }

        # è¨ˆç®—æ•´é«”çµ±è¨ˆ
        if self.results:
            all_metrics = []
            for result in self.results:
                if result["status"] == "success":
                    all_metrics.append(result["metrics"])

            if all_metrics:
                # è¨ˆç®—å¹³å‡å€¼
                avg_metrics = {}
                keys = ["Mean Glucose", "TIR", "TAR", "TBR", "CV", "GMI", "GRI"]

                for key in keys:
                    values = [m.get(key, np.nan) for m in all_metrics]
                    values = [v for v in values if not np.isnan(v)]
                    if values:
                        avg_metrics[key] = float(np.mean(values))

                summary["average_metrics"] = avg_metrics

        # å„²å­˜ç¸½çµ
        summary_file = os.path.join(self.config["output_dir"], "batch_summary.json")
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)

        return summary


def main():
    """ä¸»å‡½æ•¸"""
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•ï¼š")
        print("  python batch_process.py <file_pattern> [config.yaml]")
        print("\nç¯„ä¾‹ï¼š")
        print("  python batch_process.py '*.csv'")
        print("  python batch_process.py 'data/*.csv' config.yaml")
        print("  python batch_process.py '/path/to/cgm_*.csv'")
        sys.exit(1)

    file_pattern = sys.argv[1]
    config_file = sys.argv[2] if len(sys.argv) > 2 else None

    print("=" * 60)
    print("CGM æ‰¹æ¬¡è™•ç†å·¥å…·")
    print("=" * 60)

    # å‰µå»ºè™•ç†å™¨
    processor = BatchProcessor(config_file)

    # åŸ·è¡Œæ‰¹æ¬¡è™•ç†
    result = processor.process_files(file_pattern)

    # é¡¯ç¤ºç¸½çµ
    if result.get("average_metrics"):
        print("\nğŸ“Š æ•´é«”å¹³å‡æŒ‡æ¨™ï¼š")
        for key, value in result["average_metrics"].items():
            print(f"  - {key}: {value:.1f}")

    print(f"\næ‰€æœ‰å ±å‘Šå·²å„²å­˜è‡³ï¼š{result['output_directory']}")

    return 0 if result["failed"] == 0 else 1


if __name__ == "__main__":
    sys.exit(main())